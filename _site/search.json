[
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex042/Hands-on_Ex042.html",
    "href": "Hands-on_Ex/Hands-on_Ex042/Hands-on_Ex042.html",
    "title": "Hands-on Exercise 4.2",
    "section": "",
    "text": "A point estimate is a single number, such as a mean.\nUncertainty is expressed as standard error, confidence interval, or credible interval\nImportant:\n\nDon’t confuse the uncertainty of a point estimate with the variation in the sample\n\n\n\npacman::p_load(tidyverse, plotly, crosstalk, DT, ggdist, gganimate, dplyr, ggplot2)\n\n\nexam <- read_csv(\"data/Exam_data.csv\")\n\n\n\nThe code chunk below performs the followings:\n\ngroup the observation by RACE,\ncomputes the count of observations, mean, standard deviation and standard error of Maths by RACE, and\nsave the output as a tibble data table called my_sum.\n\n\nmy_sum <- exam %>%\n  group_by(RACE) %>%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %>%\n  mutate(se=sd/sqrt(n-1))\n\nNext, the code chunk below will\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n \n  \n    RACE \n    n \n    mean \n    sd \n    se \n  \n \n\n  \n    Chinese \n    193 \n    76.50777 \n    15.69040 \n    1.132357 \n  \n  \n    Indian \n    12 \n    60.66667 \n    23.35237 \n    7.041005 \n  \n  \n    Malay \n    108 \n    57.44444 \n    21.13478 \n    2.043177 \n  \n  \n    Others \n    9 \n    69.66667 \n    10.72381 \n    3.791438 \n  \n\n\n\n\n\n\n\n\nThe code chunk below is used to reveal the standard error of mean maths score by race.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean \n          maths score by rac\")\n\n\n\n\n\n\n\n\n# calculate the upper and lower bounds of the confidence interval\nmy_sum <- my_sum %>%\n  mutate(lower = mean - 1.96 * se, upper = mean + 1.96 * se)\n\n# plot the data with error bars sorted by mean score\nggplot(my_sum, aes(x = mean, y = reorder(RACE, mean), xmin = lower, xmax = upper)) +\n  geom_errorbar(height = 0.2, colour = \"black\", alpha = 0.9, size = 0.5) +\n  geom_point(color = \"red\", size = 1.5, alpha = 1) +\n  ggtitle(\"95% Confidence Interval of Mean Math Score by Race\") +\n  xlab(\"Mean Math Score\") +\n  ylab(\"Race\") +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\n\n\n\n# calculate the upper and lower bounds of the confidence interval\nmy_sum <- my_sum %>%\n  mutate(lower = mean - 2.58 * se, upper = mean + 2.58 * se)\n\n# plot the data with error bars sorted by mean score\nggplot(my_sum, aes(x = mean, y = reorder(RACE, mean), xmin = lower, xmax = upper)) +\n  geom_errorbar(height = 0.2, colour = \"black\", alpha = 0.9, size = 0.5) +\n  geom_point(color = \"red\", size = 1.5, alpha = 1) +\n  ggtitle(\"95% Confidence Interval of Mean Math Score by Race\") +\n  xlab(\"Mean Math Score\") +\n  ylab(\"Race\") +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n\n\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +   #<<\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\nexam %>%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(\n    show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\n\n\n\n\n\n\n\nStep 1: Installing ungeviz package\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\nStep 2: Launch the application in R\n\nlibrary(ungeviz)\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)\n\n\n\n\n\n\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "In-class_Ex/In-Class_Ex04/In-Class_Ex04.html",
    "href": "In-class_Ex/In-Class_Ex04/In-Class_Ex04.html",
    "title": "In-Class Exercise 4",
    "section": "",
    "text": "exam_data <- read_csv(\"data/Exam_data.csv\")\n\n\nggplot(exam_data,\n       aes(sample=ENGLISH))+\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that the points deviate significantly from the straight diagonal line. This is a clear indication that the set of data is not normally distributed.\n\n\n\nqq <- ggplot(exam_data,\n       aes(sample=ENGLISH))+\n  stat_qq() +\n  stat_qq_line()\n\n\nsw_t <- exam_data %>%\n  shapiro_test(ENGLISH) %>%\n  gt()\n\n\ntmp <- tempfile(fileext = '.png')\ngtsave(sw_t, tmp)\ntable_png <- png::readPNG(tmp, native = TRUE)\nqq + table_png\n\n\n\n\n\nset.seed(1234)\n\ngghistostats(\n  data = exam_data,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  vlab = \"English scores\"\n)\n\n\n\n\n\ngghistostats(\n  data = exam_data,\n  x = ENGLISH,\n  type = \"p\",\n  test.value = 60,\n  vlab = \"English scores\"\n)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Unveiling Demographic and Financial Characteristics of City of Engagement: A Data-driven Study",
    "section": "",
    "text": "City of Engagement, with a total population of 50,000, is a small city located at Country of Nowhere. The city serves as a service centre of an agriculture region surrounding the city. The main agriculture of the region is fruit farms and vineyards. The local council of the city is in the process of preparing the Local Plan 2023. A sample survey of 1000 representative residents had been conducted to collect data related to their household demographic and spending patterns, among other things. The city aims to use the data to assist with their major community revitalization efforts, including how to allocate a very large city renewal grant they have recently received."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-cleaning",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-cleaning",
    "title": "Unveiling Demographic and Financial Characteristics of City of Engagement: A Data-driven Study",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nFirst, we will have to start with missing value check on both datasets.\n\n# Missing value check for financial_journal dataset\nmissing_values_fj <- sapply(Journal, function(x) sum(is.na(x)))\nprint(missing_values_fj)\n\nparticipantId     timestamp      category        amount \n            0             0             0             0 \n\n# Missing value check for participants dataset\nmissing_values_participants <- sapply(participants, function(x) sum(is.na(x)))\nprint(missing_values_participants)\n\n participantId  householdSize       haveKids            age educationLevel \n             0              0              0              0              0 \n interestGroup      joviality \n             0              0 \n\n\nSeems no missing value appears in the dataset, we them move on to the next part.\nNext part is checking for duplicate participant IDs: Ensure that each participant ID is unique and there are no duplicates. I use the duplicated() function to identify and remove any duplicate rows.\n\n# Checking for duplicate participant IDs\nduplicate_ids <- participants[duplicated(participants$participantId), \"participantId\"]\nif (length(duplicate_ids) > 0) {\n  print(paste(\"Duplicate participant IDs found:\", paste(duplicate_ids, collapse = \", \")))\n} else {\n  print(\"No duplicate participant IDs found.\")\n}\n\n[1] \"Duplicate participant IDs found: numeric(0)\"\n\n\nNext, I will perform categorical variable standardization, the levels of categorical variables like “educationLevel” or “interestGroup” might inconsistent capitalization or spelling, I decided to use functions like tolower() or recode() from the dplyr to standarde them for consistency and easier analysis.\n\nlibrary(dplyr)\n\n# Standardizing the educationLevel variable\nparticipants <- participants %>%\n  mutate(educationLevel = recode(educationLevel,\n                                 \"Low\" = \"Low\",\n                                 \"HighSchoolOrCollege\" = \"High School or College\",\n                                 \"Bachelors\" = \"Bachelors\",\n                                 \"Graduate\" = \"Graduate\"))\n\n# Standardizing the interestGroup variable\nparticipants <- participants %>%\n  mutate(interestGroup = recode(interestGroup,\n                                \"A\" = \"Group A\",\n                                \"B\" = \"Group B\",\n                                \"C\" = \"Group C\",\n                                \"D\" = \"Group D\",\n                                \"E\" = \"Group E\",\n                                \"F\" = \"Group F\",\n                                \"G\" = \"Group G\",\n                                \"H\" = \"Group H\",\n                                \"I\" = \"Group I\",\n                                \"J\" = \"Group J\"))\n\nparticipants <- participants %>%\n  mutate(haveKids = if_else(haveKids == \"TRUE\", TRUE, FALSE)) # Convert \"TRUE\" and \"FALSE\" to boolean TRUE and FALSE\n\nNext, we need to validate the “timestamp” variable in the “financial_journal” dataset, so I converted the “timestamp” column to a proper datetime format\n\nJournal$timestamp <- as.POSIXct(Journal$timestamp, format = \"%Y-%m-%d %H:%M:%S\")\n\nNow all the seems to be cleaned, lets take a quick look at each table:\n\nhead(Journal)\n\n# A tibble: 6 × 4\n  participantId timestamp           category  amount\n          <dbl> <dttm>              <chr>      <dbl>\n1             0 2022-03-01 00:00:00 Wage      2473. \n2             0 2022-03-01 00:00:00 Shelter   -555. \n3             0 2022-03-01 00:00:00 Education  -38.0\n4             1 2022-03-01 00:00:00 Wage      2047. \n5             1 2022-03-01 00:00:00 Shelter   -555. \n6             1 2022-03-01 00:00:00 Education  -38.0\n\n\n\nhead(participants)\n\n# A tibble: 6 × 7\n  participantId householdSize haveKids   age educationLevel        interestGroup\n          <dbl>         <dbl> <lgl>    <dbl> <chr>                 <chr>        \n1             0             3 TRUE        36 High School or Colle… Group H      \n2             1             3 TRUE        25 High School or Colle… Group B      \n3             2             3 TRUE        35 High School or Colle… Group A      \n4             3             3 TRUE        21 High School or Colle… Group I      \n5             4             3 TRUE        43 Bachelors             Group H      \n6             5             3 TRUE        32 High School or Colle… Group D      \n# ℹ 1 more variable: joviality <dbl>\n\n\nNow that the data cleaning steps have been completed, we can proceed with Exploratory Data Analysis (EDA)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#descriptive-statistics",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#descriptive-statistics",
    "title": "Unveiling Demographic and Financial Characteristics of City of Engagement: A Data-driven Study",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\nTo compute descriptive statistics for numerical variables in R, we can use the summary() function.\nHere’s how I create plots for the variables in the participants dataframe and the Journal dataframe using the ggplot2 package in R:\n\n# Histogram for age\nhistogram_age <- ggplot(participants, aes(x = age)) +\n  geom_histogram(fill = \"lightblue\", color = \"black\") +\n  labs(title = \"Distribution of Age\", x = \"Age\", y = \"Count\")\n\n# Bar plot for educationLevel\nbarplot_education <- ggplot(participants, aes(x = educationLevel)) +\n  geom_bar(fill = \"lightblue\", color = \"black\") +\n  labs(title = \"Distribution of Education Level\", x = \"Education Level\", y = \"Count\")\n\n# Bar plot for interestGroup\nbarplot_interest <- ggplot(participants, aes(x = interestGroup)) +\n  geom_bar(fill = \"lightblue\", color = \"black\") +\n  labs(title = \"Distribution of Interest Group\", x = \"Interest Group\", y = \"Count\")\n\n# Calculate median joviality for each age group\nmedian_joviality <- participants %>%\n  group_by(age) %>%\n  summarize(median_joviality = median(joviality))\n\n# Plot median joviality by age group\nscatterplot_age_joviality <- ggplot(median_joviality, aes(x = age, y = median_joviality)) +\n  geom_line() +\n  labs(title = \"Median Joviality by Age Group\", x = \"Age\", y = \"Median Joviality\")\n\n# Arrange plots in a grid\ngrid.arrange(histogram_age, barplot_education, barplot_interest, scatterplot_age_joviality,\n             nrow = 2, ncol = 2)\n\n\n\n\nFor the Age, the distribution seems very rather smooth with no clear peak or hump of a group age, it jumps up and down in between ages with no concentrated point.\nFor the Education level, people who have high school or college level appears to be most common.\nFor the interest group and median joviality, either of them appear to have any normal distribution symptom.\n\n# Box plot for amount\nboxplot_amount <- ggplot(Journal, aes(x = \"\", y = amount)) +\n  geom_boxplot(fill = \"lightblue\", color = \"black\", outlier.shape = NA) +\n  coord_cartesian(ylim = c(-10, 50)) +  # Set the y-axis limits as desired\n  labs(title = \"Distribution of Amount\", x = \"\", y = \"Amount\")\n\n# Bar plot for category\nbarplot_category <- ggplot(Journal, aes(x = category)) +\n  geom_bar(fill = \"lightblue\", color = \"black\") +\n  labs(title = \"Distribution of Expense Categories\", x = \"Category\", y = \"Count\")\n\n# Plot total amount spent per month\nJournal$month <- floor_date(Journal$timestamp, \"month\")\nmonthly_totals <- Journal %>%\n  group_by(month) %>%\n  summarize(total_amount = sum(amount))\n\nlineplot_total_amount <- ggplot(monthly_totals, aes(x = month, y = total_amount)) +\n  geom_line() +\n  labs(title = \"Total Amount Spent per Month\", x = \"Month\", y = \"Total Amount\")\n\n# Arrange plots in a grid\ngrid.arrange(boxplot_amount, barplot_category, lineplot_total_amount,\n             nrow = 2, ncol = 2)\n\n\n\n\nFor the amount, as it concentrated towards the 0, it means the amount of transactions are mostly small.\nFor the Expense category, the transaction categories are mostly on food, recreation, and wages.\nFor the total amount spent per month, except for before April 2022, all other time spam seems quite smooth with transaction amount."
  }
]